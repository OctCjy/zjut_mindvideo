{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Fairmot介绍\n",
    "  FairMOT是由华中科技大学和微软亚洲研究院提出的多目标跟踪（MOT）baseline，作者分析现存one-shot目标追踪算法的问题所在，提出了三个观点：\n",
    "（1）anchors对于Re-ID并不友好，应该采用anchor-free算法。\n",
    "（2）多层特征的融合。\n",
    "（3）对于one-shot方法，Re-ID的特征向量采用低维度更好。\n",
    "  Fairmot网络架构在MOT15、MOT16、MOT17、MOT20等数据集上以30fps的帧数达到了目前的SOTA水平。\n",
    "  多目标跟踪一直是计算机视觉的一个长期目标，目标是估计视频中多个目标的轨迹，该任务的成功解决将有利于许多应用，如动作识别、运动视频分析、老年护理和人机交互。现存的SOTA方法当中大部分都是采用two-step方法两步走：\n",
    "（1）通过目标检测算法检测到目标。\n",
    "（2）再经过Re-ID模型进行匹配并根据特征上定义的特定度量将其链接到一个现有的轨迹。\n",
    "  尽管随着近年来目标检测算法与Re-ID的发展，two-step方法在目标跟踪上也有明显的性能提升，但是two-step方法不会共享检测算法与Re-ID的特征图，所以其速度很慢，很难在视频速率下进行推理。随着two-step方法的成熟，更多的研究人员开始研究同时检测目标和学习Re-ID特征的one-shot算法，当特征图在目标检测与Re-ID之间共享之后，可以大大的减少推理时间，但在精度上就会比two-step方法低很多。所以作者针对one-shot方法进行分析，提出了上述三个方面的因素。\n",
    "  一些SOTA的跟踪算法通常是two-step算法，他们将检测目标和Re-ID分成了两个任务：\n",
    "（1）首先通过检测算法获取到物体的位置(预测框)。\n",
    "（2）将预测的物体裁剪下来进行缩放传入身份特征提取器来获取Re-ID特征，连接框形成多条轨迹。\n",
    "  连接框形成轨迹的标准做法就是：根据Re-ID特征和框的IOU来计算一个代价矩阵，再利用卡尔曼滤波和匈牙利算法实现连接轨迹的任务。有一小部分研究使用了更复杂的关联策略，如群体模型和RNNs。\n",
    "  two-step方法的好处就是，可以在两个任务当中分别使用合适的模型，并且可以将预测的框进行裁剪和缩放传入Re-ID特征提取器当中，有助于处理对象比例变化。并且跟踪效果也很好，但是速度很慢，难以以视频速率进行推理。\n",
    "  One-shot方法核心思想是在一个网络中同时完成目标检测和身份嵌入(Re-ID feature)，通过共享大部分计算量来减少推理时间。\n",
    "（1）Track-RCNN通过添加一个Re-ID head的部分为每个候选区域来回归框和Re-ID的部分。\n",
    "（2）JDE则是实现在YOLOV3框架的基础上并实现了视频速率的推理。\n",
    "  然而，单步one-shot方法的跟踪精度往往低于two-step跟踪方法。论文发现这是因为学习的ReID特性不是最优的，这导致了大量的ID切换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.细节\n",
    "  Fairmot框架如下，首先将输入图像送入编码器-解码器网络，以提取高分辨率特征图（步长=4）；然后添加两个简单的并行 head，分别预测边界框和 Re-ID 特征；最后提取预测目标中心处的特征进行边界框时序联结。\n",
    "  \n",
    "  ![图片](./Fig1.jpg)\n",
    "  \n",
    "  采用 anchor-free 目标检测方法，估计高分辨率特征图上的目标中心。去掉锚点这一操作可以缓解歧义问题，使用高分辨率特征图可以帮助 Re-ID 特征与目标中心更好地对齐。\n",
    "  添加并行分支来估计像素级 Re-ID 特征，这类特征用于预测目标的 ID。具体而言，学习既能减少计算时间又能提升特征匹配稳健性的低维 Re-ID 特征。在这一步中，Fairmot用深层聚合算子（Deep Layer Aggregation，DLA）来改进主干网络 ResNet-34 ，从而融合来自多个层的特征，处理不同尺度的目标。\n",
    "## 主干网络\n",
    "  采用ResNet-34 作为主干网络，以便在准确性和速度之间取得良好的平衡。为了适应不同规模的对象，将深层聚合（DLA）的一种变体应用于主干网络。\n",
    "  与原始DLA 不同，它在低层聚合和低层聚合之间具有更多的跳跃连接，类似于特征金字塔网络（FPN）。此外，上采样模块中的所有卷积层都由可变形的卷积层代替，以便它们可以根据对象的尺寸和姿势动态调整感受野。 这些修改也有助于减轻对齐问题。\n",
    "  \n",
    "## 物体检测分支\n",
    "  Fairmot将目标检测视为高分辨率特征图上基于中心的包围盒回归任务。特别是将三个并行回归头（regression heads）附加到主干网络以分别估计热图，对象中心偏移和边界框大小。 通过对主干网络的输出特征图应用3×3卷积（具有256个通道）来实现每个回归头（head），然后通过1×1卷积层生成最终目标。\n",
    "（1）Heatmap Head：该head负责估计对象中心的位置。这里采用基于热图的表示法，热图的尺寸为1×H×W。 随着热图中位置和对象中心之间的距离，响应呈指数衰减。\n",
    "（2）Center Offset Head：该head负责更精确地定位对象。ReID功能与对象中心的对齐精准度对于性能至关重要。\n",
    "（3）Box Size Head：该部分负责估计每个锚点位置的目标边界框的高度和宽度，与Re-ID功能没有直接关系，但是定位精度将影响对象检测性能的评估。\n",
    "## ID嵌入分支 Identity Embedding Branch\n",
    "  id嵌入分支的目标是生成可以区分不同对象的特征。理想情况下，不同对象之间的距离应大于同一对象之间的距离。为了实现该目标，Fairmot在主干特征之上应用了具有128个内核的卷积层，以提取每个位置的身份嵌入特征。\n",
    "  \n",
    "## 损失函数\n",
    "（1）Heatmap loss:Fairmot按照高斯分布将物体的中心映射到了heatmap上，然后使用变形的focal loss进行预测的heatmap和实际真实的heatmap损失函数的求解，公式如下：\n",
    "$$ L_{heatmap}=-\\frac{1}{N}\\sum_{xy}{\\left\\{\\begin{array}{c}\n",
    "\\left(1-\\hat{M}_{xy}\\right)^{\\alpha}\\log\\left(\\hat{M}_{xy}\\right),if\\,\\,M_{xy}=1\\\\\n",
    "\\left(1-\\hat{M}_{xy}\\right)^{\\beta}\\left(\\hat{M}_{xy}\\right)^{\\alpha}\\log\\left(1-\\hat{M}_{xy}\\right),otherwise\\\\ \\end{array}\\right.}$$\n",
    "$\\hat{M}_{xy}$是预测的heatmap特征图，$M_xy$是heatmap的ground-truth,$N$为一个图中物体总数量。\n",
    "（2）Offset and Size loss:Fairmot用了两个L1损失就实现了Offset和Size损失：\n",
    "$$L_{box}=\\sum_{i=1}^N{\\left\\|o^i-\\hat{o}^i-\\hat{o}^i\\right\\|_1-\\left\\|S^i-\\hat{S}^i\\right\\|_1}$$\n",
    "其中，$N$为一个图中物体总数量，$S$表示Size 框的大小，$O$表示Offset 中心点的偏差。\n",
    "（3）Identity Embedding Loss：FairMOT中的Embedding也是需要借助分类（按照物体ID为不同物体分配不同的类别）进行学习的。其中分类用到softmax损失：\n",
    "$$L_{identity}=-\\sum_{i=1}^N{\\sum_{k=1}^K{L^i\\left(k\\right)\\log\\left(p\\left(k\\right)\\right)}}$$\n",
    "其中，$N$为一个图中物体总数量，$K$是类别数量。即，这部分需要对图片中每个物体进行分类识别，这里分类识别是具体认识到是指那一个物体，具有相同身份的所有对象实例都被视为一个类。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. 环境准备\n",
    "本案例基于MindSpore实现，开始实验前，请确保本地已经安装了mindspore、download、pycocotools、opencv-python、Cython、cython-bbox、decord等环境和python库\n",
    "\n",
    "并且安装mindvideo安装包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://gitee.com/yanlq46462828/zjut_mindvideo.git\n",
    "cd zjut_mindvideo\n",
    "\n",
    "Please first install mindspore according to instructions on the official website: https://www.mindspore.cn/install\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据准备与处理\n",
    "FairMot 模型使用混合数据集在此存储库中进行训练和验证。 我们在这部分使用训练数据作为 JDE，我们称之为“MIX”。请参考他们的 DATA ZOO 下载并准备所有训练数据，包括 Caltech Pedestrian、CityPersons、CUHK-SYSU、PRW、ETHZ、MOT17 和 MOT16。然后将所有训练和评估数据放入一个目录，然后将 data.json 中的“data_root”更改为该目录，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，使用目录 ./src/data/builder.py文件中的build_transforms函数对视频进行transforms pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transforms(cfg):\n",
    "    \"\"\" build data transform pipeline. \"\"\"\n",
    "    cfg_pipeline = cfg\n",
    "    if not isinstance(cfg_pipeline, list):\n",
    "        return ClassFactory.get_instance_from_cfg(cfg_pipeline,\n",
    "                                                  ModuleType.PIPELINE)\n",
    "\n",
    "    transforms = []\n",
    "    for transform in cfg_pipeline:\n",
    "        transform_op = build_transforms(transform)\n",
    "        transforms.append(transform_op)\n",
    "\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.使用说明\n",
    "  在基于Mindspore框架下的Fairmot的baseline代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "import numpy as np\n",
    "from mindspore import nn\n",
    "from mindspore import ops\n",
    "\n",
    "from src.utils.check_param import Rel, Validator\n",
    "from src.utils.class_factory import ClassFactory, ModuleType\n",
    "from src.models.layers import DeformConv2d, FairMOTMultiHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Cell):\n",
    "    \"\"\"\n",
    "    DLA中的残差快\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout, stride=1, dilation=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv_bn_act=nn.Conv2dBnAct(cin,cout,kernel_size=3, stride=stride, pad_mode='pad', padding=dilation, has_bias=False,         dilation=dilation, has_bn=True, momentum=0.9,ctivation='relu', after_fake=False)\n",
    "        self.conv_bn = nn.Conv2dBnAct(cout, cout, kernel_size=3, stride=1, pad_mode='same',\n",
    "                                      has_bias=False, dilation=dilation, has_bn=True,\n",
    "                                      momentum=0.9, activation=None)\n",
    "        self.relu = ops.ReLU()\n",
    "\n",
    "    def construct(self, x, residual=None):\n",
    "        if residual is None:\n",
    "            residual = x\n",
    "        out = self.conv_bn_act(x)\n",
    "        out = self.conv_bn(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Root(nn.Cell):\n",
    "    \"\"\"\n",
    "获取HDA节点\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, residual):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=1, has_bias=False,\n",
    "                              pad_mode='pad', padding=(kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = ops.ReLU()\n",
    "        self.residual = residual\n",
    "        self.cat = ops.Concat(axis=1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        children = x\n",
    "        x = self.conv(self.cat(x))\n",
    "        x = self.bn(x)\n",
    "        if self.residual:\n",
    "            x += children[0]\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6241ffbbadf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\"\n\u001b[0;32m      3\u001b[0m     \u001b[0m构建深度聚合网络\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Tree(nn.Cell):\n",
    "    \"\"\"\n",
    "    构建深度聚合网络.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, levels, block, in_channels, out_channels, stride=1, level_root=False,\n",
    "                 root_dim=0, root_kernel_size=1, dilation=1, root_residual=False):\n",
    "        super(Tree, self).__init__()\n",
    "        self.levels = levels\n",
    "        if root_dim == 0:\n",
    "            root_dim = 2 * out_channels\n",
    "        if level_root:\n",
    "            root_dim += in_channels\n",
    "        if self.levels == 1:\n",
    "            self.tree1 = block(in_channels, out_channels, stride, dilation=dilation)\n",
    "            self.tree2 = block(out_channels, out_channels, 1, dilation=dilation)\n",
    "        else:\n",
    "            self.tree1 = Tree(levels - 1, block, in_channels, out_channels, stride, root_dim=0,\n",
    "                              root_kernel_size=root_kernel_size,dilation=dilation, root_residual=root_residual)\n",
    "            self.tree2 = Tree(levels - 1, block, out_channels, out_channels, root_dim=root_dim +out_channels,root_kernel_size=root_kernel_size, dilation=dilation, root_residual=root_residual)\n",
    "        if self.levels == 1:\n",
    "            self.root = Root(root_dim, out_channels, root_kernel_size, root_residual)\n",
    "        self.level_root = level_root\n",
    "        self.root_dim = root_dim\n",
    "        self.downsample = None\n",
    "        self.project = None\n",
    "        if stride > 1:\n",
    "            self.downsample = nn.MaxPool2d(stride, stride=stride)\n",
    "        if in_channels != out_channels:\n",
    "            self.project = nn.Conv2dBnAct(in_channels, out_channels, kernel_size=1, stride=1, pad_mode='same',has_bias=False, has_bn=True, momentum=0.9,\n",
    "                        activation=None, after_fake=False)\n",
    "\n",
    "    def construct(self, x, residual=None, children=None):\n",
    "    \n",
    "        children = () if children is None else children\n",
    "        bottom = self.downsample(x) if self.downsample else x\n",
    "        residual = self.project(bottom) if self.project else bottom\n",
    "        if self.level_root:\n",
    "            children += (bottom,)\n",
    "        x1 = self.tree1(x, residual)\n",
    "        if self.levels == 1:\n",
    "            x2 = self.tree2(x1)\n",
    "            ida_node = (x2, x1) + children\n",
    "            x = self.root(ida_node)\n",
    "        else:\n",
    "            children += (x1,)\n",
    "            x = self.tree2(x1, children=children)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLA34(nn.Cell):\n",
    "    \"\"\"\n",
    "    构建下采样深度聚合网络\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, levels, channels, block=None, residual_root=False):\n",
    "        super(DLA34, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.base_layer=nn.Conv2dBnAct(3, channels[0], kernel_size=7, stride=1, pad_mode='same',has_bias=False, has_bn=True, momentum=0.9, activation='relu', after_fake=False)\n",
    "        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])\n",
    "        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)\n",
    "        self.level2 = Tree(levels[2], block, channels[1], channels[2], 2,\n",
    "                           level_root=False, root_residual=residual_root)\n",
    "        self.level3 = Tree(levels[3], block, channels[2], channels[3], 2,\n",
    "                           level_root=True, root_residual=residual_root)\n",
    "        self.level4 = Tree(levels[4], block, channels[3], channels[4], 2,\n",
    "                           level_root=True, root_residual=residual_root)\n",
    "        self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n",
    "                           level_root=True, root_residual=residual_root)\n",
    "        self.dla_fn = [self.level0, self.level1, self.level2, self.level3, self.level4, self.level5]\n",
    "\n",
    "    def _make_conv_level(self, cin, cout, convs, stride=1, dilation=1):\n",
    "        modules = []\n",
    "        for i in range(convs):\n",
    "            modules.append(nn.Conv2dBnAct(cin, cout, kernel_size=3, stride=stride if i == 0 else 1, pad_mode='pad', padding=dilation, has_bias=False, dilation=dilation, has_bn=True, momentum=0.9, activation='relu', after_fake=False))\n",
    "            cin = cout\n",
    "        return nn.SequentialCell(modules)\n",
    "\n",
    "    def construct(self, x):\n",
    "        y = []\n",
    "        x = self.base_layer(x)\n",
    "        for i in range(len(self.channels)):\n",
    "            x = self.dla_fn[i](x)\n",
    "            y.append(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DlaDeformConv(nn.Cell):\n",
    "    \"\"\"\n",
    "    具有bn和relu的可变形卷积v2。.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout):\n",
    "        super(DlaDeformConv, self).__init__()\n",
    "        self.actf = nn.SequentialCell([\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU()])\n",
    "        self.conv = DeformConv2d(cin, cout, kernel_size=3, stride=1, has_bias=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.actf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDAUp(nn.Cell):\n",
    "    \"\"\"IDA上采样.\"\"\"\n",
    "\n",
    "    def __init__(self, o, channels, up_f):\n",
    "        super(IDAUp, self).__init__()\n",
    "        proj_list = []\n",
    "        up_list = []\n",
    "        node_list = []\n",
    "        for i in range(1, len(channels)):\n",
    "            c = channels[i]\n",
    "            f = int(up_f[i])\n",
    "            proj = DlaDeformConv(c, o)\n",
    "            node = DlaDeformConv(o, o)\n",
    "            up = nn.Conv2dTranspose(o, o, f * 2, stride=f, pad_mode='pad', padding=f // 2,                                                                                                                                  group=o)\n",
    "            proj_list.append(proj)\n",
    "            up_list.append(up)\n",
    "            node_list.append(node)\n",
    "        self.proj = nn.CellList(proj_list)\n",
    "        self.up = nn.CellList(up_list)\n",
    "        self.node = nn.CellList(node_list)\n",
    "\n",
    "    def construct(self, layers, startp, endp):\n",
    "        for i in range(startp + 1, endp):\n",
    "            upsample = self.up[i - startp - 1]\n",
    "            project = self.proj[i - startp - 1]\n",
    "            layers[i] = upsample(project(layers[i]))\n",
    "            node = self.node[i - startp - 1]\n",
    "            layers[i] = node(layers[i] + layers[i - 1])\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLAUp(nn.Cell):\n",
    "    \"\"\"DLA上采样.\"\"\"\n",
    "    def __init__(self, startp, channels, scales, in_channels=None):\n",
    "        super(DLAUp, self).__init__()\n",
    "        self.startp = startp\n",
    "        channels = list(channels)\n",
    "        if in_channels is None:\n",
    "            in_channels = list(channels)\n",
    "        scales = np.array(scales, dtype=int)\n",
    "        self.ida = []\n",
    "        for i in range(len(channels) - 1):\n",
    "            j = -i - 2\n",
    "            self.ida.append(IDAUp(channels[j], in_channels[j:],\n",
    "                                  scales[j:] // scales[j]))\n",
    "            scales[j + 1:] = scales[j]\n",
    "            in_channels[j + 1:] = [channels[j] for _ in channels[j + 1:]]\n",
    "        self.ida_nfs = nn.CellList(self.ida)\n",
    "\n",
    "    def construct(self, layers):\n",
    "        out = [layers[-1]]  # start with 32\n",
    "        for i in range(len(layers) - self.startp - 1):\n",
    "            ida = self.ida_nfs[i]\n",
    "            layers = ida(layers, len(layers) - i - 2, len(layers))\n",
    "            out.append(layers[-1])\n",
    "        a = []\n",
    "        i = len(out)\n",
    "        while i > 0:\n",
    "            a.append(out[i - 1])\n",
    "            i -= 1\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ClassFactory.register(ModuleType.MODEL)\n",
    "class DLASegConv(nn.Cell):\n",
    "    \"\"\"\n",
    "    DLA的backbone网络\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 down_ratio: int,\n",
    "                 last_level: int,\n",
    "                 out_channel: int = 0,\n",
    "                 stage_levels: Tuple[int] = (1, 1, 1, 2, 2, 1),\n",
    "                 stage_channels: Tuple[int] = (16, 32, 64, 128, 256, 512)):\n",
    "        super(DLASegConv, self).__init__()\n",
    "        Validator.check('down_ratio', down_ratio, 'given_ratio', [2, 4, 8, 16], rel=Rel.IN)\n",
    "        self.first_level = int(np.log2(down_ratio))\n",
    "        self.last_level = last_level\n",
    "        self.base = DLA34(stage_levels, stage_channels, block=BasicBlock)\n",
    "        channels = stage_channels\n",
    "        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n",
    "        self.dla_up = DLAUp(self.first_level, channels[self.first_level:], scales)\n",
    "        if out_channel == 0:\n",
    "            out_channel = channels[self.first_level]\n",
    "        self.ida_up = IDAUp(out_channel, channels[self.first_level:self.last_level],\n",
    "                            [2 ** i for i in range(self.last_level - self.first_level)])\n",
    "\n",
    "    def construct(self, image):\n",
    "        x = self.base(image)\n",
    "        x = self.dla_up(x)\n",
    "        y = []\n",
    "        for i in range(self.last_level - self.first_level):\n",
    "            y.append(x[i])\n",
    "        y = self.ida_up(y, 0, len(y))\n",
    "        return y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassFactory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@ClassFactory\u001b[39m\u001b[38;5;241m.\u001b[39mregister(ModuleType\u001b[38;5;241m.\u001b[39mMODEL)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFairmotDla34\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mCell):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    TODO: Fairmot网络.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      8\u001b[0m                  down_ratio: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      9\u001b[0m                  last_level: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m                  feature_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     15\u001b[0m                  reg: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ClassFactory' is not defined"
     ]
    }
   ],
   "source": [
    "@ClassFactory.register(ModuleType.MODEL)\n",
    "class FairmotDla34(nn.Cell):\n",
    "    \"\"\"\n",
    "    TODO: Fairmot网络.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 down_ratio: int = 4,\n",
    "                 last_level: int = 5,\n",
    "                 head_channel: int = 256,\n",
    "                 head_conv2_ksize: Union[int, Tuple[int]] = 1,\n",
    "                 hm: int = 1,\n",
    "                 wh: int = 4,\n",
    "                 feature_id: int = 128,\n",
    "                 reg: int = 2):\n",
    "        super().__init__()\n",
    "        backbone_output_channel = 64\n",
    "        self.backbone = DLASegConv(down_ratio=down_ratio,\n",
    "                                   last_level=last_level)\n",
    "        self.head = FairMOTMultiHead(heads={'hm': hm, 'wh': wh, 'feature_id': feature_id, 'reg': reg},in_channel=backbone_output_channel,head_conv=head_channel,kernel_size=head_conv2_ksize)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练过程\n",
    "（1）首先，准备数据集。如上述所说，准备“MIX”数据集作为训练。随后，定义先验框。最后，对数据集进行数据增强。整体流程可见代码，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# perpare dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transforms \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_transforms\u001b[49m(config\u001b[38;5;241m.\u001b[39mdata_loader\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;241m.\u001b[39moperations)\n\u001b[0;32m      3\u001b[0m data_set \u001b[38;5;241m=\u001b[39m build_dataset(config\u001b[38;5;241m.\u001b[39mdata_loader\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m      4\u001b[0m data_set\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transforms\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_transforms' is not defined"
     ]
    }
   ],
   "source": [
    "    # perpare dataset\n",
    "    transforms = build_transforms(config.data_loader.train.map.operations)\n",
    "    data_set = build_dataset(config.data_loader.train.dataset)\n",
    "    data_set.transform = transforms\n",
    "    dataset_train = data_set.run()\n",
    "    Validator.check_int(dataset_train.get_dataset_size(), 0, Rel.GT)\n",
    "    batches_per_epoch = dataset_train.get_dataset_size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）设定网络框架。网络框架如上述所述，因此这里直接定义网络架构。并且定义loss，学习率、优化器等超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# set network\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m(config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# set loss\u001b[39;00m\n\u001b[0;32m      5\u001b[0m network_loss \u001b[38;5;241m=\u001b[39m build_loss(config\u001b[38;5;241m.\u001b[39mloss)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "    # set network\n",
    "    network = build_model(config.model)\n",
    "\n",
    "    # set loss\n",
    "    network_loss = build_loss(config.loss)\n",
    "    # set lr\n",
    "    lr_cfg = config.learning_rate\n",
    "    lr_cfg.steps_per_epoch = int(batches_per_epoch / config.data_loader.group_size)\n",
    "    lr = get_lr(lr_cfg)\n",
    "\n",
    "    # set optimizer\n",
    "    config.optimizer.params = network.trainable_params()\n",
    "    config.optimizer.learning_rate = lr\n",
    "    network_opt = build_optimizer(config.optimizer)\n",
    "\n",
    "    if config.train.pre_trained:\n",
    "        # load pretrain model\n",
    "        param_dict = load_checkpoint(config.train.pretrained_model)\n",
    "        load_param_into_net(network, param_dict)\n",
    "\n",
    "    # set checkpoint for the network\n",
    "    ckpt_config = CheckpointConfig(\n",
    "        save_checkpoint_steps=config.train.save_checkpoint_steps,\n",
    "        keep_checkpoint_max=config.train.keep_checkpoint_max)\n",
    "    ckpt_callback = ModelCheckpoint(prefix=config.model_name,\n",
    "                                    directory=ckpt_save_dir,\n",
    "                                    config=ckpt_config)\n",
    "\n",
    "    # init the whole Model\n",
    "    model = Model(network,\n",
    "                  network_loss,\n",
    "                  network_opt,\n",
    "                  metrics={\"Accuracy\": Accuracy()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）最后，开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loading cuhksysu...\n",
    "cuhksysu loaded.\n",
    "Loading caltech...\n",
    "caltech loaded.\n",
    "Loading citypersons...\n",
    "citypersons loaded.\n",
    "Loading mot17...\n",
    "mot17 loaded.\n",
    "Loading prw...\n",
    "prw loaded.\n",
    "Loading eth...\n",
    "eth loaded.\n",
    "[Start training `fairmot_dla34`]\n",
    "================================================================================\n",
    "epoch: 1 step: 1, loss is 55.63408279418945\n",
    "epoch: 1 step: 2, loss is 135.11972045898438\n",
    "epoch: 1 step: 3, loss is 68.24212646484375\n",
    "epoch: 1 step: 4, loss is 99.36243438720703\n",
    "epoch: 1 step: 5, loss is 77.19902801513672\n",
    "epoch: 1 step: 6, loss is 61.66413116455078\n",
    "epoch: 1 step: 7, loss is 51.42118453979492\n",
    "epoch: 1 step: 8, loss is 39.20842742919922\n",
    "epoch: 1 step: 9, loss is 62.66177749633789\n",
    "epoch: 1 step: 10, loss is 68.33760070800781\n",
    "epoch: 1 step: 11, loss is 64.51337432861328\n",
    "epoch: 1 step: 12, loss is 66.97705841064453\n",
    "epoch: 1 step: 13, loss is 85.49947357177734\n",
    "epoch: 1 step: 14, loss is 44.306880950927734\n",
    "epoch: 1 step: 15, loss is 44.892669677734375\n",
    "epoch: 1 step: 16, loss is 32.04488754272461\n",
    "epoch: 1 step: 17, loss is 53.46352005004883\n",
    "epoch: 1 step: 18, loss is 33.23221206665039\n",
    "epoch: 1 step: 19, loss is 94.40752410888672\n",
    "epoch: 1 step: 20, loss is 42.513668060302734\n",
    "epoch: 1 step: 21, loss is 53.83503341674805\n",
    "epoch: 1 step: 22, loss is 71.90801239013672\n",
    "epoch: 1 step: 23, loss is 83.15853881835938\n",
    "epoch: 1 step: 24, loss is 36.20119857788086\n",
    "epoch: 1 step: 25, loss is 35.76030731201172\n",
    "epoch: 1 step: 26, loss is 48.240718841552734\n",
    "epoch: 1 step: 27, loss is 40.638771057128906\n",
    "epoch: 1 step: 28, loss is 30.805248260498047\n",
    "epoch: 1 step: 29, loss is 60.74918746948242\n",
    "epoch: 1 step: 30, loss is 55.86394500732422\n",
    "epoch: 1 step: 31, loss is 39.79429626464844\n",
    "epoch: 1 step: 32, loss is 36.09943771362305\n",
    "epoch: 1 step: 33, loss is 41.27968215942383\n",
    "epoch: 1 step: 34, loss is 43.07084274291992\n",
    "epoch: 1 step: 35, loss is 32.99536895751953\n",
    "epoch: 1 step: 36, loss is 52.2248649597168\n",
    "epoch: 1 step: 37, loss is 35.28694534301758\n",
    "epoch: 1 step: 38, loss is 29.907625198364258\n",
    "epoch: 1 step: 39, loss is 44.55171585083008\n",
    "epoch: 1 step: 40, loss is 36.937530517578125\n",
    "epoch: 1 step: 41, loss is 40.78886413574219\n",
    "epoch: 1 step: 42, loss is 44.26179122924805\n",
    "epoch: 1 step: 43, loss is 54.04239273071289\n",
    "epoch: 1 step: 44, loss is 66.3919677734375\n",
    "epoch: 1 step: 45, loss is 37.05625534057617\n",
    "epoch: 1 step: 46, loss is 57.69034194946289\n",
    "epoch: 1 step: 47, loss is 37.09925842285156\n",
    "epoch: 1 step: 48, loss is 41.87119674682617\n",
    "epoch: 1 step: 49, loss is 40.871116638183594\n",
    "epoch: 1 step: 50, loss is 51.75830078125\n",
    "epoch: 1 step: 51, loss is 40.27484130859375\n",
    "epoch: 1 step: 52, loss is 32.51845932006836\n",
    "epoch: 1 step: 53, loss is 65.54149627685547\n",
    "epoch: 1 step: 54, loss is 54.571102142333984\n",
    "epoch: 1 step: 55, loss is 48.70039749145508\n",
    "epoch: 1 step: 56, loss is 40.226768493652344\n",
    "epoch: 1 step: 57, loss is 40.18015670776367\n",
    "epoch: 1 step: 58, loss is 50.56803512573242\n",
    "epoch: 1 step: 59, loss is 45.177005767822266\n",
    "epoch: 1 step: 60, loss is 52.70391082763672\n",
    "epoch: 1 step: 61, loss is 44.88543701171875\n",
    "epoch: 1 step: 62, loss is 33.11354446411133\n",
    "epoch: 1 step: 63, loss is 37.11306381225586\n",
    "epoch: 1 step: 64, loss is 38.995479583740234\n",
    "epoch: 1 step: 65, loss is 47.20582580566406\n",
    "epoch: 1 step: 66, loss is 33.67197036743164\n",
    "epoch: 1 step: 67, loss is 30.655174255371094\n",
    "epoch: 1 step: 68, loss is 38.68879699707031\n",
    "epoch: 1 step: 69, loss is 64.86235046386719\n",
    "epoch: 1 step: 70, loss is 64.23455810546875\n",
    "epoch: 1 step: 71, loss is 28.83365821838379\n",
    "epoch: 1 step: 72, loss is 36.305667877197266\n",
    "epoch: 1 step: 73, loss is 32.7441520690918\n",
    "epoch: 1 step: 74, loss is 28.804264068603516\n",
    "epoch: 1 step: 75, loss is 27.86435890197754\n",
    "epoch: 1 step: 76, loss is 41.876983642578125\n",
    "epoch: 1 step: 77, loss is 31.075077056884766\n",
    "epoch: 1 step: 78, loss is 33.951351165771484\n",
    "epoch: 1 step: 79, loss is 27.698165893554688\n",
    "epoch: 1 step: 80, loss is 26.100616455078125\n",
    "epoch: 1 step: 81, loss is 42.59402847290039\n",
    "epoch: 1 step: 82, loss is 27.64974594116211\n",
    "epoch: 1 step: 83, loss is 33.34096145629883\n",
    "epoch: 1 step: 84, loss is 48.80719757080078\n",
    "epoch: 1 step: 85, loss is 36.00349807739258\n",
    "epoch: 1 step: 86, loss is 49.37395095825195\n",
    "epoch: 1 step: 87, loss is 50.31093215942383\n",
    "epoch: 1 step: 88, loss is 38.51315689086914\n",
    "epoch: 1 step: 89, loss is 30.891132354736328\n",
    "epoch: 1 step: 90, loss is 30.514766693115234\n",
    "epoch: 1 step: 91, loss is 47.496952056884766\n",
    "epoch: 1 step: 92, loss is 38.22492599487305\n",
    "epoch: 1 step: 93, loss is 30.233394622802734\n",
    "epoch: 1 step: 94, loss is 36.349761962890625\n",
    "epoch: 1 step: 95, loss is 37.37440490722656\n",
    "epoch: 1 step: 96, loss is 30.823909759521484\n",
    "epoch: 1 step: 97, loss is 51.04092025756836\n",
    "epoch: 1 step: 98, loss is 30.568363189697266\n",
    "epoch: 1 step: 99, loss is 49.097557067871094\n",
    "epoch: 1 step: 100, loss is 53.643253326416016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.评估\n",
    "在评估过程中，使用的数据集为MOT17数据集。估计结果可在\"./output\"文件中查找。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.参考内容\n",
    "\n",
    "论文：https://arxiv.org/pdf/2004.01888v2.pdf\n",
    "\n",
    "博客：\n",
    "https://blog.csdn.net/weixin_42398658/article/details/110873083\n",
    "https://blog.csdn.net/qq_41204464/article/details/122893061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
